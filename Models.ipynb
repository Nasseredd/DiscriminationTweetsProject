{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of suggested Models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data set & Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@azzamalirhabi @JihadiA8 This video of the Pes...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Oh really? No more instant restaurants? THAT'S...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @Benfrancisallen: It hasn't been a good few...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @NoToFeminism: I donâ€™t need femisnsn beca...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@MariachiMacabre 19% is not the vast majority</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Tweets  Label\n",
       "0  @azzamalirhabi @JihadiA8 This video of the Pes...    0.0\n",
       "1  Oh really? No more instant restaurants? THAT'S...    0.0\n",
       "2  RT @Benfrancisallen: It hasn't been a good few...    0.0\n",
       "3  RT @NoToFeminism: I donâ€™t need femisnsn beca...    0.0\n",
       "4      @MariachiMacabre 19% is not the vast majority    0.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#file = 'Data/data.csv'\n",
    "file = 'Data/discrimant_tweet_47368_dataset.csv'\n",
    "dataframe = pd.read_csv(file, error_bad_lines=False, sep=\";\")\n",
    "dataframe = dataframe[['Tweets','Label']]\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data set's shape =  (44123, 2)\n",
      "There is 24783 missing values in the target\n"
     ]
    }
   ],
   "source": [
    "print(\"Data set's shape = \", dataframe.shape) \n",
    "print(\"There is {} missing values in the target\".format(dataframe['Label'].isnull().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    13401\n",
       "1.0     5939\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data set for a supervised learning \n",
    "df = dataframe[dataframe['Label'].isnull()==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data set's shape =  (19340, 2)\n",
      "There is 0 missing values in the target\n"
     ]
    }
   ],
   "source": [
    "print(\"Data set's shape = \", df.shape) \n",
    "print(\"There is {} missing values in the target\".format(df['Label'].isnull().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_target(y):\n",
    "    '''\n",
    "        y : Series \n",
    "    '''\n",
    "    \n",
    "    for i in range(y.shape[0]):\n",
    "        if y[i] == 'sexism' or y[i] == 'racism':\n",
    "            y[i] = 1\n",
    "        else:\n",
    "            y[i] = 0\n",
    "    return y.astype('int')\n",
    "\n",
    "def data_cleaning(df):\n",
    "    '''\n",
    "        df : DataFrame \n",
    "    '''\n",
    "    \n",
    "    import re \n",
    "    import string\n",
    "    \n",
    "    \n",
    "    i = 0 \n",
    "    for i in range(df['Tweets'].shape[0]):\n",
    "        # Remove ids @ \n",
    "        df['Tweets'][i] = re.sub(r'@\\S+', '', df['Tweets'][i])\n",
    "        \n",
    "        # Remove punctuation\n",
    "        df['Tweets'][i] = \"\".join([char for char in df['Tweets'][i] if char not in string.punctuation])\n",
    "        \n",
    "        # Uppercase -> Lowercase \n",
    "        df['Tweets'][i] = df['Tweets'][i].lower()\n",
    "        \n",
    "        # Delete Url \n",
    "        df['Tweets'][i] = re.sub(r'http\\S+', '', df['Tweets'][i])\n",
    "        \n",
    "        # Delete characters \n",
    "        df['Tweets'][i] = re.sub(\"ð|ÿ|‘|œ|¦|€|˜|™|¸|¤|‚|©|¡|…|”|“|‹|š|±|³|iâ|§|„|\", '', df['Tweets'][i]) \n",
    "        \n",
    "    return df\n",
    "\n",
    "def tokenization(df):\n",
    "    '''\n",
    "        df : DataFrame \n",
    "    '''\n",
    "    \n",
    "    # Generate tokens\n",
    "    from nltk.tokenize import TweetTokenizer\n",
    "    tknz = TweetTokenizer()\n",
    "    tokens = []\n",
    "    \n",
    "    i = 0\n",
    "    for i in range(df['Label'].shape[0]):\n",
    "        tokens.extend(tknz.tokenize(df['Tweets'][i]))\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def stemming(tokens):\n",
    "    '''\n",
    "        tokens : list \n",
    "    '''\n",
    "    \n",
    "    from nltk.stem import PorterStemmer\n",
    "    stemming = PorterStemmer()\n",
    "    \n",
    "    for token in tokens:\n",
    "        token = stemming.stem(token)\n",
    "    return tokens\n",
    "\n",
    "def tokens_frequencies(tokens):\n",
    "    '''\n",
    "        tokens : list \n",
    "    ''' \n",
    "    # Creation of a dataframe Tokens-Frequencies\n",
    "    from nltk.probability import FreqDist\n",
    "    fdist = FreqDist()\n",
    "    \n",
    "    for token in tokens:\n",
    "        fdist[token] += 1 \n",
    "    tokens_freq = pd.DataFrame(list(fdist.items()), columns = [\"Tokens\",\"Frequencies\"])\n",
    "    \n",
    "    # Sort the dataframe according to frequency of words\n",
    "    tokens_freq.sort_values(by='Frequencies',ascending=False, inplace=True)\n",
    "    \n",
    "    return tokens_freq\n",
    "\n",
    "def stop_words(df):\n",
    "    '''\n",
    "        df : DataFrame\n",
    "    '''\n",
    "    from nltk.corpus import stopwords\n",
    "    \n",
    "    liste = []\n",
    "    i = 0 \n",
    "    for i in range(df.shape[0]):\n",
    "        if df['Tokens'][i] not in stopwords.words('english'):\n",
    "            liste.append([df['Tokens'][i],df['Frequencies'][i]])\n",
    "    return pd.DataFrame(liste,columns=[\"Tokens\",\"Frequencies\"])\n",
    "\n",
    "def bag_of_words(df, nbr_tokens, token_frequency):\n",
    "    from nltk.tokenize import TweetTokenizer \n",
    "\n",
    "    # Most frequent tokens\n",
    "    most_freq = token_frequency['Tokens'][:nbr_tokens]\n",
    "\n",
    "    # Vectorization \n",
    "    matrix = []\n",
    "    for tweet in df['Tweets']:\n",
    "        vector = []\n",
    "        tknz = TweetTokenizer()\n",
    "        tweet = tknz.tokenize(tweet)\n",
    "        for token in most_freq:\n",
    "            if token in tweet:\n",
    "                vector.append(1)\n",
    "            else:\n",
    "                vector.append(0)\n",
    "        matrix.append(vector)\n",
    "\n",
    "    # Convert the matrix into a dataframe\n",
    "    matrix = pd.DataFrame(matrix, columns=most_freq)\n",
    "\n",
    "    return matrix\n",
    "\n",
    "def tfidf(df, nbr_tokens, token_frequency, ngram):\n",
    "    from tqdm import tqdm\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "    #tf_idf = TfidfVectorizer(ngram_range=ngram,\n",
    "                         #binary=True,\n",
    "                         #smooth_idf=False, \n",
    "                         #max_features=nbr_tokens )#nbr_tokens)\n",
    "    tf_idf = TfidfVectorizer(max_features=nbr_tokens,\n",
    "                             binary=True,\n",
    "                             smooth_idf=False,\n",
    "                             min_df=5, \n",
    "                             max_df=0.7, \n",
    "                                )\n",
    "\n",
    "    return tf_idf.fit_transform(np.array(df['Tweets'])).toarray()\n",
    "\n",
    "def hashing(df, nbr_tokens):\n",
    "\n",
    "    from sklearn.feature_extraction.text import HashingVectorizer\n",
    "    vectorizer = HashingVectorizer(n_features=nbr_tokens)\n",
    "    \n",
    "    return vectorizer.transform(df['Tweets']).toarray()\n",
    "\n",
    "def word2vec(df, nbr_tokens, token_frequency):\n",
    "    pass \n",
    "\n",
    "\n",
    "def vectorization(df, nbr_tokens, token_frequency, method):\n",
    "    '''\n",
    "        df : DataFrame \n",
    "        nbr_tokens : int - the number of tokens from the token-frequency DataFrame  \n",
    "        token_frequency : DataFrame - the array that contains the frequency of each token \n",
    "    '''\n",
    "    \n",
    "    if method == \"bow\":\n",
    "        return bag_of_words(df, nbr_tokens, token_frequency)  \n",
    "\n",
    "    elif method == \"tfidf\":\n",
    "        return tfidf(df, nbr_tokens, token_frequency, ngram=(1,1))\n",
    "        \n",
    "    elif method == \"hashing\":\n",
    "        return hashing(df, nbr_tokens) \n",
    "    \n",
    "    elif method == \"word2vec\":\n",
    "        pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_1(dataset, nbr_tokens, vectorizer):\n",
    "    '''\n",
    "        dataset : DataFrame - the raw data set \n",
    "        nbr_tokens : int - the number of tokens from the token-frequency DataFrame \n",
    "        nbr_tweets : int - the number of tweets to vectorize \n",
    "    '''\n",
    "\n",
    "    # Copy the dataset\n",
    "    df = dataset.copy()\n",
    "    y = df['Label']\n",
    "    \n",
    "    \n",
    "    # manipulations\n",
    "    df_cleaned = data_cleaning(df)\n",
    "    \n",
    "    # tokenization\n",
    "    tokens = tokenization(df_cleaned)\n",
    "    \n",
    "    # stemming\n",
    "    tokens_stemmed = stemming(tokens)\n",
    "    \n",
    "    # tokens_frequencies \n",
    "    tokfreq = tokens_frequencies(tokens_stemmed)\n",
    "    \n",
    "    # Stop words \n",
    "    tokfreq = stop_words(tokfreq)\n",
    "    \n",
    "    # Generate a CSV file for Tokens-Frequencies\n",
    "    tokfreq.to_csv(\"Word-Frenquency.csv\")\n",
    "    \n",
    "    # vectorization\n",
    "    X = vectorization(df, nbr_tokens, tokfreq, vectorizer)\n",
    "    \n",
    "    # Encoding target \n",
    "    #y = encoding_target(y)\n",
    "    print(y.value_counts())\n",
    "\n",
    "    # Split the data : Train set & Test set \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42, shuffle=True)\n",
    "    \n",
    "    return pd.DataFrame(X_train), pd.DataFrame(X_test), pd.DataFrame(y_train), pd.DataFrame(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0    6842\n",
      "1.0    3158\n",
      "Name: Label, dtype: int64\n",
      "CPU times: user 10.7 s, sys: 593 ms, total: 11.3 s\n",
      "Wall time: 11.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "X_train, X_test, y_train, y_test = preprocessing_1(df[:10000], 100, \"tfidf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.490747</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.411156</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0    1         2    3    4    5    6    7    8    9   ...   90   91   92  \\\n",
       "0  0.0  0.0  0.490747  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "1  0.0  0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "2  0.0  0.0  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "\n",
       "    93   94   95   96        97   98   99  \n",
       "0  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  \n",
       "1  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  \n",
       "2  0.0  0.0  0.0  0.0  0.411156  0.0  0.0  \n",
       "\n",
       "[3 rows x 100 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy of Models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Support Vector Machine (SVM)** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def support_vector_machine(X_train, X_test, y_train, y_test):\n",
    "    # training \n",
    "    from sklearn.svm import SVC \n",
    "    model = SVC(kernel='linear', random_state=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # prediction \n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # evaluation \n",
    "    from sklearn.metrics import accuracy_score #, classification_report\n",
    "    return accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.7846666666666666\n",
      "CPU times: user 4.78 s, sys: 60.6 ms, total: 4.84 s\n",
      "Wall time: 4.86 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"Accuracy = \", support_vector_machine(X_train, X_test, y_train, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convolution Neural Network (CNN)** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolution_neural_network(X_train, X_test, y_train, y_test):\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(8, input_dim=X_train.shape[1], activation='relu'))\n",
    "    model.add(Dense(4, activation='relu'))\n",
    "    model.add(Dense(1, input_dim=X_train.shape[1], activation='sigmoid'))\n",
    "    #print(model.summary())\n",
    "    model.compile(loss='binary_crossentropy', optimizer='rmsprop',metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=4, verbose=1)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "    # evaluation \n",
    "    from sklearn.metrics import accuracy_score #, classification_report\n",
    "    return accuracy_score(y_test, y_pred.round()) # round because y_pred is a vector of probabilites "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 0.6275 - accuracy: 0.6827 - val_loss: 0.5893 - val_accuracy: 0.6860\n",
      "Epoch 2/4\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.5707 - accuracy: 0.6904 - val_loss: 0.5556 - val_accuracy: 0.7347\n",
      "Epoch 3/4\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.5387 - accuracy: 0.7491 - val_loss: 0.5322 - val_accuracy: 0.7637\n",
      "Epoch 4/4\n",
      "219/219 [==============================] - 0s 1ms/step - loss: 0.5128 - accuracy: 0.7700 - val_loss: 0.5139 - val_accuracy: 0.7717\n",
      "Accuracy =  0.7716666666666666\n",
      "CPU times: user 2.02 s, sys: 220 ms, total: 2.24 s\n",
      "Wall time: 1.64 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "print(\"Accuracy = \", convolution_neural_network(X_train, X_test, y_train, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reccurent Neural Network (RNN) : Long Short-Term Memory (LSTM)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def long_short_term_memory(X_train, X_test, y_train, y_test):\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense, LSTM\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(32, batch_input_shape=(32,1,X_train.shape[1]), return_sequences=False))\n",
    "    model.add(Dense(1))\n",
    "    print(model.summary())\n",
    "    model.compile(loss='binary_crossentropy', optimizer='rmsprop',metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=4)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    \n",
    "    # evaluation \n",
    "    from sklearn.metrics import accuracy_score \n",
    "    return accuracy_score(y_test, y_pred.round()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#long_short_term_memory(X_train, X_test, y_train, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
