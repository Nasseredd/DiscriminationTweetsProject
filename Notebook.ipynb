{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "October 19th 2020 \n",
    "\n",
    "Status : \n",
    "* Remove IDs column Remove Twitter Ids, Remove punctuation, Convert uppercase into lowercase, Remove special characters, Delete URLs, Tokenization, Stemming, Generate token's frequencies, Stop words, First simple vectorization (bag of words), Create a \"delete emojis\" fonction, First modelisation with SVM, First Evaluation with f1 score.  \n",
    "\n",
    "Further : \n",
    "* Develop a modular programming \n",
    "* Develop an evaluation function (include an evolution graphic) \n",
    "* Create a sub-branch & Conduct unit tests of each function in it \n",
    "* Explore others methods of vectorization - Implement the BERT method \n",
    "* Figure out the most import tokens in tweets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string \n",
    "import re \n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem import PorterStemmer \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'Data/data.csv'\n",
    "dataframe = pd.read_csv(file, error_bad_lines=False, sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_target(y):\n",
    "    for i in range(y.shape[0]):\n",
    "        if y[i] == 'none':\n",
    "            y[i] = 0 \n",
    "        else:\n",
    "            y[i] = 1 \n",
    "    return y.astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning(df):\n",
    "    \n",
    "    # Delete IDs\n",
    "    df.drop('ID', axis=1, inplace=True)\n",
    "    \n",
    "    # First encoding \n",
    "    df['Label'].replace('none', 'not racist', inplace=True)\n",
    "    df['Label'].replace('racism', 'racist', inplace=True)\n",
    "    \n",
    "    i = 0 \n",
    "    for i in range(df['Tweets'].shape[0]):\n",
    "        # Remove ids @ \n",
    "        df['Tweets'][i] = re.sub(r'@\\S+', '', df['Tweets'][i])\n",
    "        \n",
    "        # Remove punctuation\n",
    "        df['Tweets'][i] = \"\".join([char for char in df['Tweets'][i] if char not in string.punctuation])\n",
    "        \n",
    "        # Uppercase -> Lowercase \n",
    "        df['Tweets'][i] = df['Tweets'][i].lower()\n",
    "        \n",
    "        # Delete Url \n",
    "        df['Tweets'][i] = re.sub(r'http\\S+', '', df['Tweets'][i])\n",
    "        \n",
    "        # Delete characters \n",
    "        df['Tweets'][i] = re.sub(\"√∞|√ø|‚Äò|≈ì|¬¶|‚Ç¨|Àú|‚Ñ¢|¬∏|¬§|‚Äö|¬©|¬°|‚Ä¶|‚Äù|‚Äú|‚Äπ|≈°|¬±|¬≥|i√¢|¬ß|‚Äû|\", '', df['Tweets'][i]) \n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Emojis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_emojis(text): \n",
    "    import emoji\n",
    "    text = emoji.demojize(text)\n",
    "    text = text.replace('_','')\n",
    "    return text.replace(':','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'winkingface winkingface grinningfacewithsweat grinningfacewithsweat'"
      ]
     },
     "execution_count": 467,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_emojis(\"üòâ üòâ üòÖ üòÖ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(df):\n",
    "    # Generate tokens\n",
    "    tknz = TweetTokenizer()\n",
    "    tokens = []\n",
    "    \n",
    "    i = 0\n",
    "    for i in range(df['Label'].shape[0]):\n",
    "        tokens.extend(tknz.tokenize(df['Tweets'][i]))\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(tokens):\n",
    "    stemming = PorterStemmer()\n",
    "    for token in tokens:\n",
    "        token = stemming.stem(token)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokens_Frequencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_frequencies(tokens):\n",
    "    \n",
    "    # Creation of a dataframe Tokens-Frequencies\n",
    "    fdist = FreqDist()\n",
    "    for token in tokens:\n",
    "        fdist[token] += 1 \n",
    "    tokens_freq = pd.DataFrame(list(fdist.items()), columns = [\"Tokens\",\"Frequencies\"])\n",
    "    \n",
    "    # Sort the dataframe according to frequency of words\n",
    "    tokens_freq.sort_values(by='Frequencies',ascending=False, inplace=True)\n",
    "    \n",
    "    return tokens_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is not used yet \n",
    "def stop_words(df):\n",
    "    from nltk.corpus import stopwords\n",
    "    liste = []\n",
    "    i = 0 \n",
    "    for i in range(df.shape[0]):\n",
    "        if df['Tokens'][i] not in stopwords.words('english'):\n",
    "            liste.append([df['Tokens'][i],df['Frequencies'][i]])\n",
    "    return pd.DataFrame(liste,columns=[\"Tokens\",\"Frequencies\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorization(df, nbr_tokens, nbr_tweets, token_frequency):\n",
    "    # Most frequent tokens\n",
    "    most_freq = token_frequency['Tokens'][:nbr_tokens]\n",
    "\n",
    "    # Vectorization \n",
    "    matrix = []\n",
    "    for tweet in df['Tweets'][:nbr_tweets]:\n",
    "        vector = []\n",
    "        tknz = TweetTokenizer()\n",
    "        tweet = tknz.tokenize(tweet)\n",
    "        for token in most_freq:\n",
    "            if token in tweet:\n",
    "                vector.append(1)\n",
    "            else:\n",
    "                vector.append(0)\n",
    "        matrix.append(vector)\n",
    "    \n",
    "    # Convert the matrix into a dataframe\n",
    "    bag_of_words = pd.DataFrame(matrix, columns=most_freq)\n",
    "    \n",
    "    return bag_of_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(dataset, nbr_tokens, nbr_tweets):\n",
    "    \n",
    "    # Copy the dataset\n",
    "    df = dataset.copy()\n",
    "    \n",
    "    # manipulations\n",
    "    df_cleaned = data_cleaning(df)\n",
    "    \n",
    "    # tokenization\n",
    "    tokens = tokenization(df_cleaned)\n",
    "    \n",
    "    # stemming\n",
    "    tokens_stemmed = stemming(tokens)\n",
    "    \n",
    "    # tokens_frequencies \n",
    "    tokfreq = tokens_frequencies(tokens_stemmed)\n",
    "    \n",
    "    # Stop words \n",
    "    tokfreq = stop_words(tokfreq)\n",
    "    \n",
    "    # Generate a CSV file for Tokens-Frequencies\n",
    "    tokfreq.to_csv(\"Word-Frenquency.csv\")\n",
    "    \n",
    "    # vectorization\n",
    "    X = vectorization(df, nbr_tokens, nbr_tweets, tokfreq)\n",
    "    \n",
    "    # Encoding target \n",
    "    y = dataframe.iloc[:,-1]\n",
    "    y = encoding_target(y)\n",
    "    \n",
    "    # Split the data : Train set & Test set \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = preprocessing(dataframe, nbr_tokens=100, nbr_tweets=dataframe.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC \n",
    "model = SVC(kernel='linear')\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score =  0.39191564147627417\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "print(\"F1 Score = \", f1_score(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
