{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nasser Branch \n",
    "**October 10th :** \n",
    "Tokenization, Frequency of tokens $\\rightarrow$ csv file \n",
    "\n",
    "**October 14th :**\n",
    "Remove IDs column, Remove punctuation, Convert uppercase into lowercase, Remove special characters, \n",
    "Tokenization, Stemming, Generate token's frequencies, First simple vectorization (bag of words)\n",
    "\n",
    "**October 17th :**\n",
    "Remove Twitter Ids,Delete URLs, Stop words, Create a \"delete emojis\" fonction, First modelisation with Support Vevctor Machine, First Evaluation with f1 score. \n",
    "\n",
    "**October 20th :** Add the specification of each function, Develop a modular programming, Develop an evaluation function (include an evolution graphic).\n",
    "\n",
    "**October 25th :** Evaluation function test on 2000 most frequent tokens \n",
    "\n",
    "**November 7th :** (Evaluation function update) Train Logistic Regression, Support Vector Machine, Random Forest, Decision Tree and Adaboost.  \n",
    "\n",
    "**Further :**\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'Data/data.csv'\n",
    "dataframe = pd.read_csv(file, error_bad_lines=False, sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing functions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Encoding target**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_target(y):\n",
    "    '''\n",
    "        y : Series \n",
    "    '''\n",
    "    \n",
    "    for i in range(y.shape[0]):\n",
    "        if y[i] == 'sexism' or y[i] == 'racism':\n",
    "            y[i] = 1\n",
    "        else:\n",
    "            y[i] = 0\n",
    "    return y.astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert Emoji**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert emoji \n",
    "def convert_emojis(text): \n",
    "    '''\n",
    "        text : string \n",
    "    '''\n",
    "\n",
    "    import emoji\n",
    "    \n",
    "    text = emoji.demojize(text)\n",
    "    text = text.replace('_','')\n",
    "    return text.replace(':','')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data cleaning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning(df):\n",
    "    '''\n",
    "        df : DataFrame \n",
    "    '''\n",
    "    \n",
    "    import re \n",
    "    import string\n",
    "   \n",
    "    \n",
    "    # Delete IDs\n",
    "    df.drop('ID', axis=1, inplace=True)\n",
    "    \n",
    "    # First encoding \n",
    "    df['Label'].replace('none', 'not racist', inplace=True)\n",
    "    df['Label'].replace('racism', 'racist', inplace=True)\n",
    "    \n",
    "    i = 0 \n",
    "    for i in range(df['Tweets'].shape[0]):\n",
    "        # Remove ids @ \n",
    "        df['Tweets'][i] = re.sub(r'@\\S+', '', df['Tweets'][i])\n",
    "        \n",
    "        # Remove punctuation\n",
    "        df['Tweets'][i] = \"\".join([char for char in df['Tweets'][i] if char not in string.punctuation])\n",
    "        \n",
    "        # Uppercase -> Lowercase \n",
    "        df['Tweets'][i] = df['Tweets'][i].lower()\n",
    "        \n",
    "        # Delete Url \n",
    "        df['Tweets'][i] = re.sub(r'http\\S+', '', df['Tweets'][i])\n",
    "        \n",
    "        # Delete characters \n",
    "        df['Tweets'][i] = re.sub(\"ð|ÿ|‘|œ|¦|€|˜|™|¸|¤|‚|©|¡|…|”|“|‹|š|±|³|iâ|§|„|\", '', df['Tweets'][i]) \n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(df):\n",
    "    '''\n",
    "        df : DataFrame \n",
    "    '''\n",
    "    \n",
    "    # Generate tokens\n",
    "    from nltk.tokenize import TweetTokenizer\n",
    "    tknz = TweetTokenizer()\n",
    "    tokens = []\n",
    "    \n",
    "    i = 0\n",
    "    for i in range(df['Label'].shape[0]):\n",
    "        tokens.extend(tknz.tokenize(df['Tweets'][i]))\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stemming**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(tokens):\n",
    "    '''\n",
    "        tokens : list \n",
    "    '''\n",
    "    \n",
    "    from nltk.stem import PorterStemmer\n",
    "    stemming = PorterStemmer()\n",
    "    \n",
    "    for token in tokens:\n",
    "        token = stemming.stem(token)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokens frequencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_frequencies(tokens):\n",
    "    '''\n",
    "        tokens : list \n",
    "    ''' \n",
    "    # Creation of a dataframe Tokens-Frequencies\n",
    "    from nltk.probability import FreqDist\n",
    "    fdist = FreqDist()\n",
    "    \n",
    "    for token in tokens:\n",
    "        fdist[token] += 1 \n",
    "    tokens_freq = pd.DataFrame(list(fdist.items()), columns = [\"Tokens\",\"Frequencies\"])\n",
    "    \n",
    "    # Sort the dataframe according to frequency of words\n",
    "    tokens_freq.sort_values(by='Frequencies',ascending=False, inplace=True)\n",
    "    \n",
    "    return tokens_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stop words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_words(df):\n",
    "    '''\n",
    "        df : DataFrame\n",
    "    '''\n",
    "    from nltk.corpus import stopwords\n",
    "    \n",
    "    liste = []\n",
    "    i = 0 \n",
    "    for i in range(df.shape[0]):\n",
    "        if df['Tokens'][i] not in stopwords.words('english'):\n",
    "            liste.append([df['Tokens'][i],df['Frequencies'][i]])\n",
    "    return pd.DataFrame(liste,columns=[\"Tokens\",\"Frequencies\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vectorization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorization(df, nbr_tokens, token_frequency):\n",
    "    '''\n",
    "        df : DataFrame \n",
    "        nbr_tokens : int - the number of tokens from the token-frequency DataFrame  \n",
    "        token_frequency : DataFrame - the array that contains the frequency of each token \n",
    "    '''\n",
    "    from nltk.tokenize import TweetTokenizer \n",
    "\n",
    "    # Most frequent tokens\n",
    "    most_freq = token_frequency['Tokens'][:nbr_tokens]\n",
    "\n",
    "    # Vectorization \n",
    "    matrix = []\n",
    "    for tweet in df['Tweets']:\n",
    "        vector = []\n",
    "        tknz = TweetTokenizer()\n",
    "        tweet = tknz.tokenize(tweet)\n",
    "        for token in most_freq:\n",
    "            if token in tweet:\n",
    "                vector.append(1)\n",
    "            else:\n",
    "                vector.append(0)\n",
    "        matrix.append(vector)\n",
    "    \n",
    "    # Convert the matrix into a dataframe\n",
    "    bag_of_words = pd.DataFrame(matrix, columns=most_freq)\n",
    "    \n",
    "    return bag_of_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing strategy 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_1(dataset, nbr_tokens):\n",
    "    '''\n",
    "        dataset : DataFrame - the raw data set \n",
    "        nbr_tokens : int - the number of tokens from the token-frequency DataFrame \n",
    "        nbr_tweets : int - the number of tweets to vectorize \n",
    "    '''\n",
    "\n",
    "    # Copy the dataset\n",
    "    df = dataset.copy()\n",
    "    y = df['Label']\n",
    "    \n",
    "    # manipulations\n",
    "    df_cleaned = data_cleaning(df)\n",
    "    \n",
    "    # tokenization\n",
    "    tokens = tokenization(df_cleaned)\n",
    "    \n",
    "    # stemming\n",
    "    tokens_stemmed = stemming(tokens)\n",
    "    \n",
    "    # tokens_frequencies \n",
    "    tokfreq = tokens_frequencies(tokens_stemmed)\n",
    "    \n",
    "    # Stop words \n",
    "    tokfreq = stop_words(tokfreq)\n",
    "    \n",
    "    # Generate a CSV file for Tokens-Frequencies\n",
    "    tokfreq.to_csv(\"Word-Frenquency.csv\")\n",
    "    \n",
    "    # vectorization\n",
    "    X = vectorization(df, nbr_tokens, tokfreq)\n",
    "    \n",
    "    # Encoding target \n",
    "    y = encoding_target(y)\n",
    "\n",
    "    # Split the data : Train set & Test set \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42, shuffle=True)\n",
    "    \n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logreg(X_train, X_test, y_train, y_test):\n",
    "    # training \n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # prediction\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # evaluation \n",
    "    from sklearn.metrics import f1_score #, classification_report\n",
    "    return f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Support Vector Machine**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def support_vector_machine(X_train, X_test, y_train, y_test):\n",
    "    # training \n",
    "    from sklearn.svm import SVC \n",
    "    model = SVC(kernel='linear', random_state=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # prediction \n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # evaluation \n",
    "    from sklearn.metrics import f1_score #, classification_report\n",
    "    return f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest(X_train, X_test, y_train, y_test):\n",
    "    # training \n",
    "    from sklearn.ensemble import RandomForestClassifier \n",
    "    model = RandomForestClassifier(random_state=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # prediction \n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # evaluation \n",
    "    from sklearn.metrics import f1_score #, classification_report\n",
    "    return f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decision Tree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_tree(X_train, X_test, y_train, y_test):\n",
    "    # training \n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    model = DecisionTreeClassifier(random_state=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # prediction \n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # evaluation \n",
    "    from sklearn.metrics import f1_score #, classification_report\n",
    "    return f1_score(y_test, y_pred) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adaboost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaboost(X_train, X_test, y_train, y_test):\n",
    "    # training \n",
    "    from sklearn.ensemble import AdaBoostClassifier\n",
    "    model = AdaBoostClassifier(random_state=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # prediction \n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # evaluation \n",
    "    from sklearn.metrics import f1_score #, classification_report\n",
    "    return f1_score(y_test, y_pred) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strategy_preprocessing(df, nbr_tokens):\n",
    "    # List of F1-score for each ML model\n",
    "    f1_logistic_regression = []\n",
    "    f1_support_vector_machine = []\n",
    "    f1_random_forest = []\n",
    "    f1_decision_tree = []\n",
    "    f1_adaboost = []\n",
    "    \n",
    "    # Generate the f1 score for each ML model depending on a certain number of tokens each time \n",
    "    for i in range(len(nbr_tokens)):\n",
    "        \n",
    "        # import a copy of data \n",
    "        df = dataframe.copy()\n",
    "        \n",
    "        # preprocessing \n",
    "        X_train, X_test, y_train, y_test = preprocessing_1(df, nbr_tokens[i]) \n",
    "\n",
    "        # training - prediction - metrics \n",
    "        f1_logistic_regression.append(logreg(X_train, X_test, y_train, y_test))\n",
    "        f1_support_vector_machine.append(support_vector_machine(X_train, X_test, y_train, y_test))\n",
    "        f1_random_forest.append(random_forest(X_train, X_test, y_train, y_test))\n",
    "        f1_decision_tree.append(decision_tree(X_train, X_test, y_train, y_test))\n",
    "        f1_adaboost.append(adaboost(X_train, X_test, y_train, y_test))\n",
    "    \n",
    "    return [f1_logistic_regression,f1_support_vector_machine,f1_random_forest,f1_decision_tree,f1_adaboost]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strategy_graphic(f1_lists, nbr_tokens):\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    colors = ['magenta','blue','cyan','green','black']\n",
    "    plt.figure(figsize=(8,8))\n",
    "    curves = []\n",
    "    labels = ['Log Reg','SVM','R.Forest','DT','Ada']\n",
    "    \n",
    "    for liste, color,l in zip(f1_lists, colors,labels):\n",
    "        curves.extend(plt.plot(nbr_tokens, liste, '-p', color, label=l))\n",
    "    \n",
    "    #plt.hlines(y = 0.7, xmin = 120, xmax = 2050, color ='r')\n",
    "    #plt.text(1, 0.7, '70%', ha ='left', va ='center')\n",
    "    \n",
    "    plt.legend(handles=curves)\n",
    "    plt.xlabel('Number of most frequent tokens')\n",
    "    plt.ylabel('F1 score')\n",
    "    plt.ylim(-0.25,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strategy_evaluation(df, nbr_tokens):\n",
    "    \n",
    "    # preprocessing + f1 score evaluation \n",
    "    f1_lists = strategy_preprocessing(df, nbr_tokens)\n",
    "    \n",
    "    # graphic \n",
    "    strategy_graphic(f1_lists, nbr_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on 200 tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [10, 20, 50,100,200,500,1000,1500,2000,2500,3000] \n",
    "\n",
    "# import \n",
    "file = 'Data/data.csv'\n",
    "dataframe = pd.read_csv(file, error_bad_lines=False, sep=\";\")\n",
    "df = dataframe[:500]\n",
    "\n",
    "strategy_evaluation(df,tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
