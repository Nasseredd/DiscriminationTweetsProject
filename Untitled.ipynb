{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string \n",
    "import re \n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem import PorterStemmer \n",
    "import nltk\n",
    "nltk.download('english')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'Data/data.csv'\n",
    "dataframe = pd.read_csv(file, error_bad_lines=False, sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning(df):\n",
    "    '''\n",
    "        df : DataFrame \n",
    "    '''\n",
    "    \n",
    "    # Delete IDs\n",
    "    df.drop('ID', axis=1, inplace=True)\n",
    "    \n",
    "    # First encoding \n",
    "    df['Label'].replace('none', 'not racist', inplace=True)\n",
    "    df['Label'].replace('racism', 'racist', inplace=True)\n",
    "    \n",
    "    i = 0 \n",
    "    for i in range(df['Tweets'].shape[0]):\n",
    "        # Remove ids @ \n",
    "        df['Tweets'][i] = re.sub(r'@\\S+', '', df['Tweets'][i])\n",
    "        \n",
    "        # Remove punctuation\n",
    "        df['Tweets'][i] = \"\".join([char for char in df['Tweets'][i] if char not in string.punctuation])\n",
    "        \n",
    "        # Uppercase -> Lowercase \n",
    "        df['Tweets'][i] = df['Tweets'][i].lower()\n",
    "        \n",
    "        # Delete Url \n",
    "        df['Tweets'][i] = re.sub(r'http\\S+', '', df['Tweets'][i])\n",
    "        \n",
    "        \n",
    "        # Delete characters \n",
    "        df['Tweets'][i] = re.sub(\"ð|ÿ|‘|œ|¦|€|˜|™|¸|¤|‚|©|¡|…|”|“|‹|š|±|³|iâ|§|„|\", '', df['Tweets'][i]) \n",
    "        \n",
    "    return df\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = data_cleaning(dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(df):\n",
    "    df['Tweets'] = df.apply(lambda row: nltk.word_tokenize(row['Tweets']), axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = tokenization(df_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lemmatization with POS Tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemma(df):\n",
    "    df['Tweets']= lemmatizer.lemmatize((w, get_wordnet_pos(w)) for w in df['Tweets'])\n",
    "    return df\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[this, video, of, the, peshmerga, decimating, ...</td>\n",
       "      <td>not racist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[oh, really, no, more, instant, restaurants, t...</td>\n",
       "      <td>not racist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[rt, it, hasnt, been, a, good, few, weeks, for...</td>\n",
       "      <td>not racist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[rt, i, donât, need, femisnsn, because, men, c...</td>\n",
       "      <td>not racist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[19, is, not, the, vast, majority]</td>\n",
       "      <td>not racist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16044</th>\n",
       "      <td>[rt, i, want, equal, rights, but, i, still, wa...</td>\n",
       "      <td>sexism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16045</th>\n",
       "      <td>[rt, go, ahead, and, call, me, sexist, but, sc...</td>\n",
       "      <td>sexism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16046</th>\n",
       "      <td>[ive, had, the, epic, but, i, always, kept, it...</td>\n",
       "      <td>not racist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16047</th>\n",
       "      <td>[so, do, you, think, that, the, daesh, are, pl...</td>\n",
       "      <td>not racist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16048</th>\n",
       "      <td>[rt, my, skin, green, no, colors, suit, only, ...</td>\n",
       "      <td>not racist</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16049 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Tweets       Label\n",
       "0      [this, video, of, the, peshmerga, decimating, ...  not racist\n",
       "1      [oh, really, no, more, instant, restaurants, t...  not racist\n",
       "2      [rt, it, hasnt, been, a, good, few, weeks, for...  not racist\n",
       "3      [rt, i, donât, need, femisnsn, because, men, c...  not racist\n",
       "4                     [19, is, not, the, vast, majority]  not racist\n",
       "...                                                  ...         ...\n",
       "16044  [rt, i, want, equal, rights, but, i, still, wa...      sexism\n",
       "16045  [rt, go, ahead, and, call, me, sexist, but, sc...      sexism\n",
       "16046  [ive, had, the, epic, but, i, always, kept, it...  not racist\n",
       "16047  [so, do, you, think, that, the, daesh, are, pl...  not racist\n",
       "16048  [rt, my, skin, green, no, colors, suit, only, ...  not racist\n",
       "\n",
       "[16049 rows x 2 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rt', 'my', 'skin', 'green', 'no', 'color', 'suit', 'only', 'wear', 'ripped', 'pant', 'because', 'hulk', 'need', 'no', 'clothes']\n"
     ]
    }
   ],
   "source": [
    "print([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in dataframe['Tweets'][16048]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
