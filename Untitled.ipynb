{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\zakof\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string \n",
    "import re \n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem import PorterStemmer \n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import warnings\n",
    "nltk.download('wordnet')\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'Data/data.csv'\n",
    "dataframe = pd.read_csv(file, error_bad_lines=False, sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning(df):\n",
    "    '''\n",
    "        df : DataFrame \n",
    "    '''\n",
    "    \n",
    "    # Delete IDs\n",
    "    df.drop('ID', axis=1, inplace=True)\n",
    "    \n",
    "    # First encoding \n",
    "    df['Label'].replace('none', 'not racist', inplace=True)\n",
    "    df['Label'].replace('racism', 'racist', inplace=True)\n",
    "    \n",
    "    i = 0 \n",
    "    for i in range(df['Tweets'].shape[0]):\n",
    "        # Remove ids @ \n",
    "        df['Tweets'][i] = re.sub(r'@\\S+', '', df['Tweets'][i])\n",
    "        \n",
    "        # Remove punctuation\n",
    "        df['Tweets'][i] = \"\".join([char for char in df['Tweets'][i] if char not in string.punctuation])\n",
    "        \n",
    "        # Uppercase -> Lowercase \n",
    "        df['Tweets'][i] = df['Tweets'][i].lower()\n",
    "        \n",
    "        # Delete Url \n",
    "        df['Tweets'][i] = re.sub(r'http\\S+', '', df['Tweets'][i])\n",
    "        \n",
    "        \n",
    "        # Delete characters \n",
    "        df['Tweets'][i] = re.sub(\"ð|ÿ|‘|œ|¦|€|˜|™|¸|¤|‚|©|¡|…|”|“|‹|š|±|³|iâ|§|„|\", '', df['Tweets'][i]) \n",
    "        \n",
    "    return df\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(df):\n",
    "    df['Tweets'] = df.apply(lambda row: nltk.word_tokenize(row['Tweets']), axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization with POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "#function to tag evry token with the corresponding POS\n",
    "def get_wordnet_pos(word):\n",
    "            \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "            tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "            tag_dict = {\"J\": wordnet.ADJ,\n",
    "                        \"N\": wordnet.NOUN,\n",
    "                        \"V\": wordnet.VERB,\n",
    "                        \"R\": wordnet.ADV}\n",
    "\n",
    "            return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Preprocessing(df):\n",
    "    #data cleaning \n",
    "    cleaned_data = data_cleaning(df)\n",
    "    \n",
    "    #tokenization \n",
    "    tokenazed_data = tokenization(cleaned_data)\n",
    "    \n",
    "    #lemmatization and remooving stop words\n",
    "    i = 0\n",
    "    for i in range(tokenazed_data['Tweets'].shape[0]):\n",
    "        tokenazed_data['Tweets'][i] = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in tokenazed_data['Tweets'][i]]\n",
    "    #remove stop words \n",
    "    \n",
    "    \n",
    "    return tokenazed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[this, video, of, the, peshmerga, decimate, is...</td>\n",
       "      <td>not racist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[oh, really, no, more, instant, restaurant, th...</td>\n",
       "      <td>not racist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[rt, it, hasnt, be, a, good, few, week, for, i...</td>\n",
       "      <td>not racist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[rt, i, donât, need, femisnsn, because, men, c...</td>\n",
       "      <td>not racist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[19, be, not, the, vast, majority]</td>\n",
       "      <td>not racist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16044</th>\n",
       "      <td>[rt, i, want, equal, right, but, i, still, wan...</td>\n",
       "      <td>sexism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16045</th>\n",
       "      <td>[rt, go, ahead, and, call, me, sexist, but, sc...</td>\n",
       "      <td>sexism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16046</th>\n",
       "      <td>[ive, have, the, epic, but, i, always, kept, i...</td>\n",
       "      <td>not racist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16047</th>\n",
       "      <td>[so, do, you, think, that, the, daesh, be, pla...</td>\n",
       "      <td>not racist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16048</th>\n",
       "      <td>[rt, my, skin, green, no, color, suit, only, w...</td>\n",
       "      <td>not racist</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16049 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Tweets       Label\n",
       "0      [this, video, of, the, peshmerga, decimate, is...  not racist\n",
       "1      [oh, really, no, more, instant, restaurant, th...  not racist\n",
       "2      [rt, it, hasnt, be, a, good, few, week, for, i...  not racist\n",
       "3      [rt, i, donât, need, femisnsn, because, men, c...  not racist\n",
       "4                     [19, be, not, the, vast, majority]  not racist\n",
       "...                                                  ...         ...\n",
       "16044  [rt, i, want, equal, right, but, i, still, wan...      sexism\n",
       "16045  [rt, go, ahead, and, call, me, sexist, but, sc...      sexism\n",
       "16046  [ive, have, the, epic, but, i, always, kept, i...  not racist\n",
       "16047  [so, do, you, think, that, the, daesh, be, pla...  not racist\n",
       "16048  [rt, my, skin, green, no, color, suit, only, w...  not racist\n",
       "\n",
       "[16049 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Preprocessing(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
