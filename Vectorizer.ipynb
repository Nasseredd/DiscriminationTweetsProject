{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization Methods "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data set and Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@azzamalirhabi @JihadiA8 This video of the Peshmerga decimating ISIS is far more interesting. ht...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Oh really? No more instant restaurants? THAT'S SHOCKING. #MKR #MKR2015</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @Benfrancisallen: It hasn't been a good few weeks for #ISIS. A new front has opened up in #Si...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @NoToFeminism: I donâ€™t need femisnsn because men carry heavy things that i cannot!!! like s...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@MariachiMacabre 19% is not the vast majority</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                Tweets  \\\n",
       "0  @azzamalirhabi @JihadiA8 This video of the Peshmerga decimating ISIS is far more interesting. ht...   \n",
       "1                               Oh really? No more instant restaurants? THAT'S SHOCKING. #MKR #MKR2015   \n",
       "2  RT @Benfrancisallen: It hasn't been a good few weeks for #ISIS. A new front has opened up in #Si...   \n",
       "3  RT @NoToFeminism: I donâ€™t need femisnsn because men carry heavy things that i cannot!!! like s...   \n",
       "4                                                        @MariachiMacabre 19% is not the vast majority   \n",
       "\n",
       "   Label  \n",
       "0    0.0  \n",
       "1    0.0  \n",
       "2    0.0  \n",
       "3    0.0  \n",
       "4    0.0  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#file = 'Data/data.csv'\n",
    "file = 'Data/discrimant_tweet_47368_dataset.csv'\n",
    "dataframe = pd.read_csv(file, error_bad_lines=False, sep=\";\")\n",
    "dataframe = dataframe[['Tweets','Label']]\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data set's shape =  (19340, 2)\n",
      "There is 0 missing values in the target\n"
     ]
    }
   ],
   "source": [
    "# Data set for a supervised learning \n",
    "df = dataframe[dataframe['Label'].isnull()==False]\n",
    "\n",
    "print(\"Data set's shape = \", df.shape) \n",
    "print(\"There is {} missing values in the target\".format(df['Label'].isnull().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_target(y):\n",
    "    '''\n",
    "        y : Series \n",
    "    '''\n",
    "    \n",
    "    for i in range(y.shape[0]):\n",
    "        if y[i] == 'sexism' or y[i] == 'racism':\n",
    "            y[i] = 1\n",
    "        else:\n",
    "            y[i] = 0\n",
    "    return y.astype('int')\n",
    "\n",
    "def data_cleaning(df):\n",
    "    '''\n",
    "        df : DataFrame \n",
    "    '''\n",
    "    \n",
    "    import re \n",
    "    import string\n",
    "    \n",
    "    \n",
    "    i = 0 \n",
    "    for i in range(df['Tweets'].shape[0]):\n",
    "        # Remove ids @ \n",
    "        df['Tweets'][i] = re.sub(r'@\\S+', '', df['Tweets'][i])\n",
    "        \n",
    "        # Remove punctuation\n",
    "        df['Tweets'][i] = \"\".join([char for char in df['Tweets'][i] if char not in string.punctuation])\n",
    "        \n",
    "        # Uppercase -> Lowercase \n",
    "        df['Tweets'][i] = df['Tweets'][i].lower()\n",
    "        \n",
    "        # Delete Url \n",
    "        df['Tweets'][i] = re.sub(r'http\\S+', '', df['Tweets'][i])\n",
    "        \n",
    "        # Delete characters \n",
    "        df['Tweets'][i] = re.sub(\"ð|ÿ|‘|œ|¦|€|˜|™|¸|¤|‚|©|¡|…|”|“|‹|š|±|³|iâ|§|„|\", '', df['Tweets'][i]) \n",
    "        \n",
    "    return df\n",
    "\n",
    "def tokenization(df):\n",
    "    '''\n",
    "        df : DataFrame \n",
    "    '''\n",
    "    \n",
    "    # Generate tokens\n",
    "    from nltk.tokenize import TweetTokenizer\n",
    "    tknz = TweetTokenizer()\n",
    "    tokens = []\n",
    "    \n",
    "    i = 0\n",
    "    for i in range(df['Label'].shape[0]):\n",
    "        tokens.extend(tknz.tokenize(df['Tweets'][i]))\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def stemming(tokens):\n",
    "    '''\n",
    "        tokens : list \n",
    "    '''\n",
    "    \n",
    "    from nltk.stem import PorterStemmer\n",
    "    stemming = PorterStemmer()\n",
    "    \n",
    "    for token in tokens:\n",
    "        token = stemming.stem(token)\n",
    "    return tokens\n",
    "\n",
    "def tokens_frequencies(tokens):\n",
    "    '''\n",
    "        tokens : list \n",
    "    ''' \n",
    "    # Creation of a dataframe Tokens-Frequencies\n",
    "    from nltk.probability import FreqDist\n",
    "    fdist = FreqDist()\n",
    "    \n",
    "    for token in tokens:\n",
    "        fdist[token] += 1 \n",
    "    tokens_freq = pd.DataFrame(list(fdist.items()), columns = [\"Tokens\",\"Frequencies\"])\n",
    "    \n",
    "    # Sort the dataframe according to frequency of words\n",
    "    tokens_freq.sort_values(by='Frequencies',ascending=False, inplace=True)\n",
    "    \n",
    "    return tokens_freq\n",
    "\n",
    "def stop_words(df):\n",
    "    '''\n",
    "        df : DataFrame\n",
    "    '''\n",
    "    from nltk.corpus import stopwords\n",
    "    \n",
    "    liste = []\n",
    "    i = 0 \n",
    "    for i in range(df.shape[0]):\n",
    "        if df['Tokens'][i] not in stopwords.words('english'):\n",
    "            liste.append([df['Tokens'][i],df['Frequencies'][i]])\n",
    "    return pd.DataFrame(liste,columns=[\"Tokens\",\"Frequencies\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization Methods "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bag of Words (BOW)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(df, nbr_tokens, token_frequency):\n",
    "    from nltk.tokenize import TweetTokenizer \n",
    "\n",
    "    # Most frequent tokens\n",
    "    most_freq = token_frequency['Tokens'][:nbr_tokens]\n",
    "\n",
    "    # Vectorization \n",
    "    matrix = []\n",
    "    for tweet in df['Tweets']:\n",
    "        vector = []\n",
    "        tknz = TweetTokenizer()\n",
    "        tweet = tknz.tokenize(tweet)\n",
    "        for token in most_freq:\n",
    "            if token in tweet:\n",
    "                vector.append(1)\n",
    "            else:\n",
    "                vector.append(0)\n",
    "        matrix.append(vector)\n",
    "\n",
    "    # Convert the matrix into a dataframe\n",
    "    matrix = pd.DataFrame(matrix, columns=most_freq)\n",
    "\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(df, nbr_tokens, token_frequency, ngram):\n",
    "    from tqdm import tqdm\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "    tf_idf = TfidfVectorizer(max_features=nbr_tokens,\n",
    "                             binary=True,\n",
    "                             smooth_idf=False,\n",
    "                             min_df=5, \n",
    "                             max_df=0.7)\n",
    "\n",
    "    return tf_idf.fit_transform(np.array(df['Tweets'])).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hashing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hashing(df, nbr_tokens):\n",
    "\n",
    "    from sklearn.feature_extraction.text import HashingVectorizer\n",
    "    vectorizer = HashingVectorizer(n_features=nbr_tokens)\n",
    "    \n",
    "    return vectorizer.transform(df['Tweets']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word2Vec** is a method to vectorize words and create new models (combinations) of words to add to features.<br> \n",
    "This method is used over bow, tf-idf or hashing. It has two main advantages : \n",
    "* Dimensionality reduction (features) \n",
    "* It capture meanings of the words, semantic relationships and context \n",
    "\n",
    "Word2Vec is a combination of two shallow neural networks : **CBOW** & **Skip-gram**. <br> \n",
    "&emsp;**CBOW** tends to predict the probability of a word given a context. <br> \n",
    "&emsp;**Skip-gram** model tries to predict the context for a given word (reverse manner). <br> \n",
    "\n",
    "Word2Vec needs a pre-trained word vector and here are the most known : <br> \n",
    "&emsp;Google News Word Vectors <br> \n",
    "&emsp;Freebase names <br> \n",
    "&emsp;DBPedia vectors (wiki2vec) <br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec(df, nbr_tokens, token_frequency):\n",
    "    \n",
    "    from gensim.models import Word2Vec\n",
    "    \n",
    "    model = Word2Vec(\n",
    "            token_frequency['Tokens'],\n",
    "            size = 200, \n",
    "            #window = 5,                   # context window size\n",
    "            #min_count = 2,                # Ignores all words with total frequency lower than 2.                                  \n",
    "            #sg = 1,                       # 1 for skip-gram model\n",
    "            #hs = 0,\n",
    "            #negative = 10,                # for negative sampling\n",
    "            #workers= 32,                  # no.of cores\n",
    "            seed = 34) \n",
    "\n",
    "    model.train(token_frequency['Tokens'], total_examples= len(df['Tweets']), epochs=10)\n",
    "    return model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vectorization function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorization(df, nbr_tokens, token_frequency, method):\n",
    "    '''\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    if method == \"bow\":\n",
    "        return bag_of_words(df, nbr_tokens, token_frequency)  \n",
    "\n",
    "    elif method == \"tfidf\":\n",
    "        return tfidf(df, nbr_tokens, token_frequency, ngram=(1,1))\n",
    "        \n",
    "    elif method == \"hashing\":\n",
    "        return hashing(df, nbr_tokens) \n",
    "    \n",
    "    elif method == \"word2vec\":\n",
    "        return word2vec(df, nbr_tokens, token_frequency) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(dataset, nbr_tokens, vectorizer):\n",
    "    '''\n",
    "        \n",
    "    '''\n",
    "\n",
    "    # Copy the dataset\n",
    "    df = dataset.copy()\n",
    "    y = df['Label']\n",
    "    \n",
    "    \n",
    "    # manipulations\n",
    "    df_cleaned = data_cleaning(df)\n",
    "    \n",
    "    # tokenization\n",
    "    tokens = tokenization(df_cleaned)\n",
    "    \n",
    "    # stemming\n",
    "    tokens_stemmed = stemming(tokens)\n",
    "    \n",
    "    # tokens_frequencies \n",
    "    tokfreq = tokens_frequencies(tokens_stemmed)\n",
    "    \n",
    "    # Stop words \n",
    "    tokfreq = stop_words(tokfreq)\n",
    "    \n",
    "    print(tokfreq)\n",
    "    \n",
    "    # Generate a CSV file for Tokens-Frequencies\n",
    "    #tokfreq.to_csv(\"Word-Frenquency.csv\")\n",
    "    \n",
    "    # vectorization\n",
    "    X = vectorization(df, nbr_tokens, tokfreq, vectorizer)\n",
    "    \n",
    "    # Encoding target \n",
    "    #y = encoding_target(y)\n",
    "    print(y.value_counts())\n",
    "\n",
    "    # Split the data : Train set & Test set \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42, shuffle=True)\n",
    "    \n",
    "    return pd.DataFrame(X_train), pd.DataFrame(X_test), pd.DataFrame(y_train), pd.DataFrame(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Tokens  Frequencies\n",
      "0             video            3\n",
      "1         peshmerga            1\n",
      "2        decimating            1\n",
      "3              isis            7\n",
      "4               far            3\n",
      "..              ...          ...\n",
      "841    hillaryemail            1\n",
      "842  hillaryclinton            1\n",
      "843            cmon            1\n",
      "844          shelli            1\n",
      "845          emilie            1\n",
      "\n",
      "[846 rows x 2 columns]\n",
      "0.0    93\n",
      "1.0    57\n",
      "Name: Label, dtype: int64\n",
      "CPU times: user 253 ms, sys: 30.6 ms, total: 284 ms\n",
      "Wall time: 278 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "X_train, X_test, y_train, y_test = preprocessing(df[:150], 10, \"hashing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizers comparison "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chosen Model : SVM** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def support_vector_machine(X_train, X_test, y_train, y_test):\n",
    "    # training \n",
    "    from sklearn.svm import SVC \n",
    "    model = SVC(kernel='linear', random_state=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # prediction \n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # evaluation \n",
    "    from sklearn.metrics import accuracy_score #, classification_report\n",
    "    return accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>**First comparison :**</u>\n",
    "* Number of Features = 20 \n",
    "* Number of Tweets = 15000\n",
    "* Same data cleaning \n",
    "* Stemming "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bag Of Words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0    10270\n",
      "1.0     4730\n",
      "Name: Label, dtype: int64\n",
      "Accuracy =  0.6922222222222222\n",
      "CPU times: user 20.3 s, sys: 835 ms, total: 21.1 s\n",
      "Wall time: 21.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "X_train, X_test, y_train, y_test = preprocessing(df[:15000], 20, \"bow\")\n",
    "print(\"Accuracy = \", support_vector_machine(X_train, X_test, y_train, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0    10270\n",
      "1.0     4730\n",
      "Name: Label, dtype: int64\n",
      "Accuracy =  0.72\n",
      "CPU times: user 19.5 s, sys: 929 ms, total: 20.4 s\n",
      "Wall time: 20.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train, X_test, y_train, y_test = preprocessing(df[:15000], 20, \"tfidf\")\n",
    "print(\"Accuracy = \", support_vector_machine(X_train, X_test, y_train, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hashing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0    10270\n",
      "1.0     4730\n",
      "Name: Label, dtype: int64\n",
      "Accuracy =  0.6922222222222222\n",
      "CPU times: user 18.1 s, sys: 752 ms, total: 18.8 s\n",
      "Wall time: 18.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train, X_test, y_train, y_test = preprocessing(df[:15000], 20, \"hashing\")\n",
    "print(\"Accuracy = \", support_vector_machine(X_train, X_test, y_train, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>**Some vectors using Word2Vec**</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using a pre-trained word vector** (GoogleNews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "model = KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin\", binary=True, limit=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary\n",
    "#list(model.wv.vocab)  # len(vocabulary) = limit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('woman', 0.7664012908935547),\n",
       " ('boy', 0.6824870109558105),\n",
       " ('teenager', 0.6586930155754089),\n",
       " ('girl', 0.5921714305877686),\n",
       " ('men', 0.5489763021469116),\n",
       " ('guy', 0.5420035123825073),\n",
       " ('person', 0.5342026352882385),\n",
       " ('Man', 0.5316052436828613),\n",
       " ('suspect', 0.5247484445571899),\n",
       " ('victim', 0.523030161857605)]"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examples \n",
    "model.wv.most_similar('man')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('military', 0.838738203048706),\n",
       " ('United_States', 0.7528706192970276),\n",
       " ('armed_forces', 0.6437433362007141),\n",
       " ('U.S.', 0.6348716020584106),\n",
       " ('civilian', 0.5731287002563477),\n",
       " ('Pentagon', 0.5701889991760254),\n",
       " ('Military', 0.5427780151367188),\n",
       " ('Afghanistan', 0.5392686128616333),\n",
       " ('Iraq', 0.526528000831604),\n",
       " ('Army', 0.520842432975769)]"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a vector from others (model)\n",
    "vect = model['United_States'] + model['military']  # Try to guess the result ? \n",
    "model.wv.most_similar([vect])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train on our data set** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         video\n",
      "1     peshmerga\n",
      "2    decimating\n",
      "3          isis\n",
      "4           far\n",
      "Name: Tokens, dtype: object\n",
      "CPU times: user 1.47 s, sys: 151 ms, total: 1.62 s\n",
      "Wall time: 1.56 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "def word2vec(df, nbr_tokens, token_frequency):\n",
    "    \n",
    "    from gensim.models import Word2Vec\n",
    "    \n",
    "    tokens = token_frequency['Tokens'].tolist()\n",
    "    \n",
    "    model = Word2Vec(\n",
    "            tokens,\n",
    "            size = 200, \n",
    "            window = 10,                   # context window size\n",
    "            min_count = 2,                # Ignores all words with total frequency lower than 2.                                  \n",
    "            #sg = 1,                       # 1 for skip-gram model\n",
    "            #hs = 0,\n",
    "            #negative = 10,                # for negative sampling\n",
    "            workers= 4,                  # no.of cores\n",
    "            seed = 34) \n",
    "\n",
    "    model.train(tokens, total_examples= len(tokens), epochs=10)\n",
    "    return model \n",
    "\n",
    "\n",
    "# Import Data - Data Cleaning - Preprocessing \n",
    "data = df[:1000].copy()\n",
    "y = data['Label']\n",
    "df_cleaned = data_cleaning(data)\n",
    "tokens = tokenization(df_cleaned)\n",
    "tokens_stemmed = stemming(tokens)\n",
    "tokfreq = tokens_frequencies(tokens_stemmed)\n",
    "tokfreq = stop_words(tokfreq)\n",
    "print(tokfreq['Tokens'].head())\n",
    "\n",
    "# Word2Vec Training \n",
    "model = word2vec(data, 1000, tokfreq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'd',\n",
       " 'e',\n",
       " 'o',\n",
       " 'p',\n",
       " 's',\n",
       " 'h',\n",
       " 'm',\n",
       " 'r',\n",
       " 'g',\n",
       " 'a',\n",
       " 'c',\n",
       " 't',\n",
       " 'n',\n",
       " 'f',\n",
       " 'l',\n",
       " 'y',\n",
       " 'k',\n",
       " 'w',\n",
       " 'â']"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#list(model.wv.vocab)\n",
    "#model.wv.most_similar(positive=\"video\")\n",
    "\n",
    "maliste = tokfreq['Tokens'][:31].tolist()\n",
    "model = Word2Vec(maliste, size = 200, window = 10, min_count = 2, workers= 4) \n",
    "model.train(maliste, total_examples= len(maliste), epochs=10)\n",
    "list(model.wv.vocab)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
