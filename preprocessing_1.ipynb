{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### def preprocessing_1(dataset, nbr_tokens):\n",
    "    \n",
    "     return X_train_vect, X_test_vect, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- cette fonction de préprocessing utilise juste pour l'instant deux métodes de vectorization\n",
    "    * le bag of word ou encore appelé one hot\n",
    "    * le tf-idf\n",
    "- pour les paramètres\n",
    "    * dataset : correspond a notre dataframe que nous chargeons a l'aide de la cmde dataset = pd.rea_csv('datset.csv',....)\n",
    "    * vectorizer : correspond a la méthode de vectorisation que nous voulons utiliser pour nos donnée\n",
    "        * 'bow' : correspond au one hot vector ou encore bag of word, tous le prétraitement est fait\n",
    "        * 'tfidf' : correspond a l'utilisation de la méthode de vectorization tf-idf et s'en suit tous le traitement\n",
    "    * nbr_tokens : correspond au nombres de stopword que nous voulons utiliser pour la representation matriceille de nos donnée, il represente le maximun de features, de paramètres a représenter\n",
    "    * sizeDataTest :  represente la taille que nous aimerions donnée a notre jeu de donnée de test dans notre dataset\n",
    "        * 10 : correspond a 10% du dataset\n",
    "        * 20 : correspond a 20%\n",
    "        * 30 : correspond a 30%\n",
    "        * vous devez choisir entre ses 3 valeurs uniquement pour l'instant\n",
    "\n",
    "- pour les sortis de notre fonction de prétraitement, elle renvoie et dans l'ordre :\n",
    "    * X_train : qui representent nos donnée d'entrainement traité et vectorisé par notre méthode choisi.\n",
    "    * X_test : qui representent nos donnée de tests traité et vectorisé par notre méthode choisi.\n",
    "    * train_labels : qui representent nos labels d'entrainement\n",
    "    * test_labels : qui representent nos labels de test tous deja converti\n",
    " \n",
    "##### NB : toutes ses sorties sont prêt a etre utilié pour entrainer un modèle comme le svm, logistic regression, naives bayes etc..\n",
    "#### Chaque processus est commenté et tracer dans l'exécution de la fonction\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_1(dataset, vectorizer, nbr_tokens, sizeDataTest):\n",
    "    #importation des bibliothèques et librairie \n",
    "    import os\n",
    "    import re\n",
    "    from tqdm import tqdm\n",
    "    import numpy as np\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    print(\"--------Importation des librairie réussie----\")\n",
    "\n",
    "\n",
    "    #importation du dataset\n",
    "    \n",
    "    #dataset = pd.read_csv(dataset, sep=',')\n",
    "    \n",
    "    #choix du BOW\n",
    "    if(vectorizer=='bow'):\n",
    "        from sklearn.feature_extraction.text import CountVectorizer\n",
    "        #instanciation de la classe countVectorizer pour le BOW\n",
    "        count_vect = CountVectorizer(max_features=nbr_tokens)\n",
    "        print(\"--------creation de la classe CountVectorizer reussite pour la vectorization----\")\n",
    "        #Definition de la taille du jeu de test\n",
    "        \n",
    "\n",
    "        if (sizeDataTest==10):\n",
    "            train_text, temp_text, train_labels, temp_labels = train_test_split(dataset['Tweet'], dataset['Label'], \n",
    "                                                                    random_state=2018, \n",
    "                                                                    test_size=0.1, \n",
    "                                                                    stratify=dataset['Label'])\n",
    "        elif(sizeDataTest==20):\n",
    "            train_text, temp_text, train_labels, temp_labels = train_test_split(dataset['Tweet'], dataset['Label'], \n",
    "                                                                    random_state=2018, \n",
    "                                                                    test_size=0.2, \n",
    "                                                                    stratify=dataset['Label'])\n",
    "        elif(sizeDataTest==30):\n",
    "            train_text, temp_text, train_labels, temp_labels = train_test_split(dataset['Tweet'], dataset['Label'], \n",
    "                                                                    random_state=2018, \n",
    "                                                                    test_size=0.3, \n",
    "                                                                    stratify=dataset['Label'])\n",
    "        else:\n",
    "            print('Le jeu de test dois être compris entre 10, 20 ou 30 pour cent du jeu de donnée')\n",
    "          \n",
    "        print(\"--------Division du dataset en jeu d'entrainement et jeu de test réussie----\")\n",
    "\n",
    "        print(\"--------Début de la vectorization avec One hot, ou encore Bah Of word----\")\n",
    "        #Vectorization des données d'entrainement (tweets) en utilisant le BOW\n",
    "        # cette etape permet de faire de la vectorization et de construire notre BOW\n",
    "        X_train_vect = count_vect.fit_transform(train_text)\n",
    "        print(\"--------Vectorization du jeu de d'entrainement réussi----\")\n",
    "        X_test_vect = count_vect.transform(temp_text)\n",
    "        print(\"--------Vectorization du jeu de test réussi----\")\n",
    "\n",
    "        #convertion en entier des labels\n",
    "        train_labels = train_labels.astype(int)\n",
    "        test_labels = temp_labels.astype(int)\n",
    "    \n",
    "    #choix d'un autre vectorizer Tf-idf\n",
    "    elif(vectorizer=='tfidf'):\n",
    "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "        import nltk\n",
    "        nltk.download(\"stopwords\")\n",
    "        from nltk.corpus import stopwords\n",
    "        print(\"--------Importation des librairie pour le Tf-idf reussie----\")\n",
    "        #Definition de la taille du jeu de test\n",
    "        if (sizeDataTest==10):\n",
    "            train_text, temp_text, train_labels, temp_labels = train_test_split(dataset['Tweet'], dataset['Label'], \n",
    "                                                                    random_state=2018, \n",
    "                                                                    test_size=0.1, \n",
    "                                                                    stratify=dataset['Label'])\n",
    "        elif(sizeDataTest==20):\n",
    "            train_text, temp_text, train_labels, temp_labels = train_test_split(dataset['Tweet'], dataset['Label'], \n",
    "                                                                    random_state=2018, \n",
    "                                                                    test_size=0.2, \n",
    "                                                                    stratify=dataset['Label'])\n",
    "        elif(sizeDataTest==30):\n",
    "            train_text, temp_text, train_labels, temp_labels = train_test_split(dataset['Tweet'], dataset['Label'], \n",
    "                                                                    random_state=2018, \n",
    "                                                                    test_size=0.3, \n",
    "                                                                    stratify=dataset['Label'])\n",
    "        else:\n",
    "            print('Le jeu de test dois être compris entre 10, 20 ou 30 pour cent du jeu de donnée')\n",
    "        \n",
    "        print(\"--------Division du dataset en jeu d'entrainement et jeu de test réussie----\")\n",
    "    \n",
    "        #parcours de tweet\n",
    "        X_train_preprocessing=[]\n",
    "        X_test_preprocessing=[]\n",
    "        \n",
    "        \n",
    "        print(\"--------Début du traitement sur les différentes phrases et mots du dataset, cette opération est un peu longue veillez patienter svp!----\")\n",
    "        print(\"---comprenez comment sa marche le tf-idf en attendant https://www.youtube.com/watch?v=G1bof7UL9RU&ab_channel=MinsukHeo%ED%97%88%EB%AF%BC%EC%84%9D\")\n",
    "        #traitement des tweets d'entrainement\n",
    "        for tweet in train_text:\n",
    "            \"\"\"\n",
    "            - transformation de maniscule en minuscule\n",
    "            - Changement de \"'t\" par \"not\"\n",
    "            - suppression \"@name\"\n",
    "            - isoler et supprimer les ponctuation sauf \"?\"\n",
    "            - supression des carractères spéciaux\n",
    "            - suppression des stopword (mots vide) a l'exception de \"not\" et \"can\"\n",
    "            - Supprimer les espaces blancs de fin\n",
    "            \"\"\"\n",
    "            #transformation de maniscule en minuscule\n",
    "            tweet = tweet.lower()\n",
    "            # Changement de \"'t\" par \"not\"\n",
    "            tweet = re.sub(r\"\\'t\", \" not\", tweet)\n",
    "            # suppression \"@name\"\n",
    "            tweet = re.sub(r'(@.*?)[\\s]', ' ', tweet)\n",
    "            # isoler et supprimer les ponctuation sauf \"?\"\n",
    "            tweet = re.sub(r'([\\'\\\"\\.\\(\\)\\!\\?\\\\\\/\\,])', r' \\1 ', tweet)\n",
    "            tweet = re.sub(r'[^\\w\\s\\?]', ' ', tweet)\n",
    "            # supression des carractères spéciaux\n",
    "            tweet = re.sub(r'([\\;\\:\\|•«\\n])', ' ', tweet)\n",
    "            # suppression des stopword (mots vide) a l'exception de \"not\" et \"can\" on peut ajouter les notres\n",
    "            tweet = \" \".join([word for word in tweet.split()\n",
    "                          if word not in stopwords.words('english')\n",
    "                          or word in ['not', 'can']])\n",
    "            # Supprimer les espaces blancs de fin\n",
    "            tweet = re.sub(r'\\s+', ' ', tweet).strip()\n",
    "            \n",
    "            #Ajout dans la liste de donnée d'entrainement\n",
    "            X_train_preprocessing.append(tweet)\n",
    "        \n",
    "        print(\"**************Prétraitement du jeu d'entraînement terminé avec succès************\")\n",
    "        print(\"- transformation de maniscule en minuscule terminé avec succès\")\n",
    "        print(\"- Changement de 't' par 'not' terminé avec succès\")\n",
    "        print(\"- - suppression des annotation '@name' terminé avec succès\")\n",
    "        print(\"- isoler et supprimer les ponctuation sauf '?' terminé avec succès\" )\n",
    "        print(\"- supression des carractères spéciaux terminé avec succès\")\n",
    "        print(\"- suppression des stopword (mots vide) a l'exception de 'not' et 'can' terminé avec succès\")\n",
    "        print(\" - Supprimer les espaces blancs de fin terminé avec succès\")\n",
    "        print(\"-----------------------------------------------------\")\n",
    "                  \n",
    "        #traitement des tweets de test\n",
    "        for tweet in temp_text:\n",
    "            \"\"\"\n",
    "            - transformation de maniscule en minuscule\n",
    "            - Changement de \"'t\" par \"not\"\n",
    "            - suppression \"@name\"\n",
    "            - isoler et supprimer les ponctuation sauf \"?\"\n",
    "            - supression des carractères spéciaux\n",
    "            - suppression des stopword (mots vide) a l'exception de \"not\" et \"can\"\n",
    "            - Supprimer les espaces blancs de fin\n",
    "            \"\"\"\n",
    "            #transformation de maniscule en minuscule\n",
    "            tweet = tweet.lower()\n",
    "            # Changement de \"'t\" par \"not\"\n",
    "            tweet = re.sub(r\"\\'t\", \" not\", tweet)\n",
    "            # suppression \"@name\"\n",
    "            tweet = re.sub(r'(@.*?)[\\s]', ' ', tweet)\n",
    "            # isoler et supprimer les ponctuation sauf \"?\"\n",
    "            tweet = re.sub(r'([\\'\\\"\\.\\(\\)\\!\\?\\\\\\/\\,])', r' \\1 ', tweet)\n",
    "            tweet = re.sub(r'[^\\w\\s\\?]', ' ', tweet)\n",
    "            # supression des carractères spéciaux\n",
    "            tweet = re.sub(r'([\\;\\:\\|•«\\n])', ' ', tweet)\n",
    "            # suppression des stopword (mots vide) a l'exception de \"not\" et \"can\" on peut ajouter les notres\n",
    "            tweet = \" \".join([word for word in tweet.split()\n",
    "                          if word not in stopwords.words('english')\n",
    "                          or word in ['not', 'can']])\n",
    "            # Supprimer les espaces blancs de fin\n",
    "            tweet = re.sub(r'\\s+', ' ', tweet).strip()\n",
    "            \n",
    "            #Ajout dans la liste de donnée d'entrainement\n",
    "            X_test_preprocessing.append(tweet)\n",
    "            \n",
    "        print(\"**************Prétraitement du jeu de test terminé avec succès************\")\n",
    "        \n",
    "        print(\"- transformation de maniscule en minuscule terminé avec succès\")\n",
    "        print(\"- Changement de 't' par 'not' terminé avec succès\")\n",
    "        print(\"- - suppression des annotation '@name' terminé avec succès\")\n",
    "        print(\"- isoler et supprimer les ponctuation sauf '?' terminé avec succès\" )\n",
    "        print(\"- supression des carractères spéciaux terminé avec succès\")\n",
    "        print(\"- suppression des stopword (mots vide) a l'exception de 'not' et 'can' terminé avec succès\")\n",
    "        print(\" - Supprimer les espaces blancs de fin terminé avec succès\")\n",
    "        print(\"-----------------------------------------------------\")\n",
    "            \n",
    "        \n",
    "        print(\"--------Début de la vectorization avec TF-IDF----\")\n",
    "        # Calcul TF-IDF\n",
    "        tf_idf = TfidfVectorizer(ngram_range=(1, 3),\n",
    "                                 binary=True,\n",
    "                                 smooth_idf=False)   \n",
    "        print(\"--------creation de la classe TfidfVectorizer reussite pour la vectorization----\")\n",
    "        #vectorization du data train et data test\n",
    "        X_train_preprocessing=np.array(X_train_preprocessing)\n",
    "        X_test_preprocessing=np.array(X_test_preprocessing)\n",
    "\n",
    "        X_train_vect = tf_idf.fit_transform(X_train_preprocessing)\n",
    "        #reutilisation de fit_transform que nous avons deja utiliser sur le jeux d'entrainement\n",
    "        print(\"--------Vectorization du jeu d'entrainement réussi----\")\n",
    "\n",
    "        \n",
    "        X_test_vect = tf_idf.transform(X_test_preprocessing)\n",
    "        print(\"--------Vectorization du jeu de test réussi----\")\n",
    "    #convertion en entier des labels\n",
    "    train_labels = train_labels.astype(int)\n",
    "    test_labels = temp_labels.astype(int)\n",
    "    print(\"--------Converssion des labels de test et d'entrainement reussie!----\")\n",
    "\n",
    "    print('')\n",
    "    print('')\n",
    "    print('')\n",
    "    print(\"***Vous avez utilisé la méhode :\", vectorizer,\"pour vectoriser votre dataset\")\n",
    "    print(\"***Vous avez divisé votre jeu de donnée en :\", sizeDataTest,\"% pour le jeu de test et :\",100-sizeDataTest,\"% pour l'entrainement\")\n",
    "    print(\"***Votre dataset à \", len(dataset),\" phrases\")\n",
    "    print(\"***Votre jeu d'entrainement à \", len(train_labels),\" phrases\")\n",
    "    print(\"***Votre jeu de test à \", len(test_labels),\" phrases\")\n",
    "\n",
    "    print(\"***********************************************\")\n",
    "    print(\"***ETAPE DE PRÉPROCESSING TERMINÉ AVEC SUCCÈS *\")\n",
    "    print(\"***********************************************\")\n",
    "    print(\"******************************************************************************************************\")\n",
    "    print(\"***Les données peuvent etre passé dans un Algo de machine learnig directement pour entrainé un modèl *\")\n",
    "    print(\"******************************************************************************************************\")\n",
    "\n",
    "    print('')\n",
    "    print('')\n",
    "    print(\"*-* utilisé le formatage suivant pour récuperer le jeu de test et d'entrainement ainsi que les labels correspondant : X_train, Xtest, y_train, y_test = preprocessing_1(dataset, 'tfidf',1000, 10)\")\n",
    "    \n",
    "    return X_train_vect, X_test_vect,train_labels,test_labels\n",
    "        \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.read_csv('discrimant_tweet_47368_data.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Importation des librairie réussie----\n",
      "--------creation de la classe CountVectorizer reussite pour la vectorization----\n",
      "--------Division du dataset en jeu d'entrainement et jeu de test réussie----\n",
      "--------Début de la vectorization avec One hot, ou encore Bah Of word----\n",
      "--------Vectorization du jeu de d'entrainement réussi----\n",
      "--------Vectorization du jeu de test réussi----\n",
      "--------Converssion des labels de test et d'entrainement reussie!----\n",
      "\n",
      "\n",
      "\n",
      "***Vous avez utilisé la méhode : bow pour vectoriser votre dataset\n",
      "***Vous avez divisé votre jeu de donnée en : 10 % pour le jeu de test et : 90 % pour l'entrainement\n",
      "***Votre dataset à  47368  phrases\n",
      "***Votre jeu d'entrainement à  42631  phrases\n",
      "***Votre jeu de test à  4737  phrases\n",
      "***********************************************\n",
      "***ETAPE DE PRÉPROCESSING TERMINÉ AVEC SUCCÈS *\n",
      "***********************************************\n",
      "******************************************************************************************************\n",
      "***Les données peuvent etre passé dans un Algo de machine learnig directement pour entrainé un modèl *\n",
      "******************************************************************************************************\n",
      "\n",
      "\n",
      "*-* utilisé le formatage suivant pour récuperer le jeu de test et d'entrainement ainsi que les labels correspondant : X_train, Xtest, y_train, y_test = preprocessing_1(dataset, 'tfidf',1000, 10)\n"
     ]
    }
   ],
   "source": [
    "X_train, Xtest, y_train, y_test = preprocessing_1(dataset, 'bow',1000, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=200)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "regressionLogistic_Classified = LogisticRegression(solver='lbfgs', max_iter=200)\n",
    "regressionLogistic_Classified.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_on_test_data = regressionLogistic_Classified.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_on_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
